# Linear-regression-with-gradient-descent
 
Linear regression with gradient descent is a statistical approach used to model the relationship between a dependent variable and one or more independent variables. The goal of linear regression is to find the best-fit line that minimizes the difference between the actual and predicted values of the dependent variable.
Gradient descent is an iterative optimization algorithm used to find the minimum of a function. In the context of linear regression, it is used to find the values of the coefficients (slope and intercept) that minimize the sum of squared errors between the actual and predicted values.
The algorithm works by starting with some initial values for the coefficients, calculating the predicted values, and then adjusting the coefficients to reduce the error between the predicted and actual values. This process is repeated until the error is minimized, and the coefficients converge to the optimal values.
The main advantages of linear regression with gradient descent are that it is computationally efficient and scalable to large datasets. It is also widely used in machine learning and statistical analysis for predicting outcomes and understanding the relationships between variables.

This repository gives an analysis of performance of linear regression models with the approaches - sklearn library, mathematical formula and gradient descent.

### Requirements
  - Python 3.x   

### Libraries:
  - numpy
  - matplotlib
